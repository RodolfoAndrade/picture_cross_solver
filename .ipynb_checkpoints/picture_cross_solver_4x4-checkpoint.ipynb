{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picture cross solver using Multilayer Neural Network\n",
    "This jupyter notebook we create an MLP for creating an image or a matrix of zeros and ones using as input an array of tips in the same style as picture cross puzzles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the dataset generated by our other jupyter notebook create_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(\"dataset/data_x_4x4.npy\")\n",
    "data_y = np.load(\"dataset/data_y_4x4.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = list(range(len(data_x)))\n",
    "random.shuffle(shuffle)\n",
    "data_x = data_x[shuffle]\n",
    "data_y = data_y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the shape of the data_y and data_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "print(data_y.shape)\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is normalized by dividing by the max value possible in the array. That is, divided by 4 (from the max size of the 4x4 matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x[0]/shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is the matrix of zeros and ones. But since we are using a tanh activation we converted the output to -1 and ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y[0]*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable set how we are going to split our training and test data. In this example, the split is one. That means that all data is set to be for training. Because we generated our own dataset including all possibilities available to the shape of the matrix, there was no need to use test set since our goal is just to solve the puzzle for the shape 4x4 matrix. If there were a huge amount of data to process as for example a 10x10 matrix then we would split the dataset so that the training would learn the general rules with a low processing cost. In this case, we only wanted to solve the puzzle, there was no need to generalize the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 1-0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_x[:int(len(data_x)*split)]/shape[0]\n",
    "test_x = data_x[int(len(data_x)*split):]/shape[0]\n",
    "train_y = data_y[:int(len(data_y)*split)]*2-1\n",
    "test_y = data_y[int(len(data_y)*split):]*2-1\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to find the best network configuration using all the available activators, optimizers and losses. We tried different combinations using a simple neural network, with only one dense layer of 16 nodes, to find the best available configuration. \n",
    "\n",
    "The result of the analysis is in another notebook called statistic. We analyze the output and determined the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"softmax\", \n",
    "               \"elu\", \n",
    "               \"selu\", \n",
    "               \"softplus\", \n",
    "               \"softsign\", \n",
    "               \"relu\", \n",
    "               \"tanh\", \n",
    "               \"sigmoid\", \n",
    "               \"hard_sigmoid\", \n",
    "               \"exponential\", \n",
    "               \"linear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\"SGD\", \n",
    "              \"RMSprop\", \n",
    "              \"Adagrad\", \n",
    "              \"Adadelta\", \n",
    "              \"Adam\", \n",
    "              \"Adamax\", \n",
    "              \"Nadam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [\"mean_squared_error\", \n",
    "          \"mean_absolute_error\", \n",
    "          \"mean_absolute_percentage_error\", \n",
    "          \"mean_squared_logarithmic_error\", \n",
    "          \"squared_hinge\", \n",
    "          \"hinge\", \n",
    "          \"categorical_hinge\", \n",
    "          \"logcosh\", \n",
    "#           \"categorical_crossentropy\", \n",
    "#           \"sparse_categorical_crossentropy\", \n",
    "          \"binary_crossentropy\", \n",
    "          \"kullback_leibler_divergence\", \n",
    "          \"poisson\", \n",
    "          \"cosine_proximity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run of different combinations, we found that nadam optimizers and cosine_proximity performed better for a simple neural network with only one layer of 16 nodes. We then used the same optimizer and loss function to train the MLP. Later on, we changed the activation function to tanh, that performed better than elu in term of accuracy.\n",
    "\n",
    "After training, we predicted all the training cases and count those that failed. In the end, we had 14591 failed cases. Close enough to the 13174 theoretical cases that would fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizers = [\"Nadam\"]\n",
    "losses = [\"cosine_proximity\"]\n",
    "activations = [\"elu\"]\n",
    "\n",
    "for i in optimizers:\n",
    "    for j in losses:\n",
    "        for l in activations:\n",
    "            print(i+\"&\"+j+\"&\"+l)\n",
    "\n",
    "            main_input = Input(shape=data_x.shape[1:], name='main_input')\n",
    "            x = Dense(shape[0]*shape[1]*12, activation=\"tanh\")(main_input)\n",
    "    #         y = keras.layers.concatenate([x, main_input])\n",
    "    #         y = Dense(shape[0]*shape[1], activation='tanh')(y)\n",
    "    #         z = keras.layers.concatenate([y, main_input])\n",
    "    #         z = Dense(shape[0]*shape[1], activation='tanh')(z)\n",
    "    #         w = keras.layers.concatenate([z, main_input])\n",
    "    #         w = Dense(shape[0]*shape[1], activation='tanh')(w)\n",
    "            x = Dense(shape[0]*shape[1], activation='tanh')(x)\n",
    "            x = Dense(shape[0]*shape[1], activation='tanh')(x)\n",
    "            main_output = Reshape(shape, name='main_output')(x)\n",
    "\n",
    "            model = Model(main_input, main_output)\n",
    "\n",
    "    #         model.summary()\n",
    "\n",
    "            model.compile(optimizer=i, loss=j, metrics=['accuracy'])\n",
    "\n",
    "    #         tensorboard = TensorBoard(log_dir=\"logs/{}\".format(i+\"_\"+j))\n",
    "\n",
    "    #         history = model.fit(train_x, train_y, epochs=20, batch_size=1, verbose=0, validation_data=(test_x, test_y), callbacks=[tensorboard])\n",
    "            history = model.fit(train_x, train_y, epochs=12800, batch_size=320, verbose=0, validation_data=(test_x, test_y))\n",
    "\n",
    "            count = 0\n",
    "            for k in range(len(train_x)):\n",
    "            #     print(i)\n",
    "            #     print(data_x[i])\n",
    "                x = train_x[k].reshape((1,-1))\n",
    "                pred = model.predict(x)\n",
    "                y = train_y[k]\n",
    "                if np.array_equal(np.round((pred+1)/2)[0],(y+1)/2) == False:\n",
    "            #         print(i)\n",
    "            #         print((y+1)/2)\n",
    "            #         print(np.round((pred+1)/2)[0])\n",
    "                    count+=1\n",
    "            print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we use a beep sound to notify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 1000  # millisecond\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the history, showing that the model continued to learn up to 12000 epochs but it did not show any sign of improvement in accuracy after that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_4x4_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print all the cases that fail to check if there are ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(data_x)):\n",
    "count = 0\n",
    "for i in range(len(train_x)):\n",
    "#     print(i)\n",
    "#     print(data_x[i])\n",
    "    x = train_x[i].reshape((1,-1))\n",
    "    pred = model.predict(x)\n",
    "    y = train_y[i]\n",
    "    if np.array_equal(np.round((pred+1)/2)[0],(y+1)/2) == False:\n",
    "        print(i)\n",
    "        print((y+1)/2)\n",
    "        print(np.round((pred+1)/2)[0])\n",
    "        count+=1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picture_cross",
   "language": "python",
   "name": "picture_cross"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
